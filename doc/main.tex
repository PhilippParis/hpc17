% !TEX root = template.tex

\newcommand\mygather{\texttt{MY\_Gather}\xspace}
\newcommand\myscatter{\texttt{MY\_Scatter}\xspace}

\newcommand\mpigather{\texttt{MPI\_Gather}\xspace}
\newcommand\mpiscatter{\texttt{MPI\_Scatter}\xspace}

%----------------------------------------------------------------------
\section{Algorithm \mygather}

%-------
\subsection{Strategy Comparison}

Two different \texttt{Gather} algorithms, one based on Binomial-Trees and one based on a Divide-And-Conquer approach were implemented and tested. Figure \ref{fig:gather:binom_vs_dac:root0} shows the performance of each implementation compared to the \texttt{Open MPI Gather} implementation with $root = 0$. Figure \ref{fig:gather:binom_vs_dac:root2} shows the same comparison with $root = 2$.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/gather_32/runtime.pdf}
        \caption{Binomial-Tree Implementation}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/gather32/runtime.pdf}
        \caption{Divide-And-Conquer Implementation}
    \end{subfigure}
    \caption{Binomial Tree and Divide-And-Conquer \texttt{Gather} with $root = 0$}
    \label{fig:gather:binom_vs_dac:root0}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/root=2/binom/gather_32/runtime.pdf}
        \caption{Binomial-Tree Implementation}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/root=2/divide_conquer/gather_32/runtime.pdf}
        \caption{Divide-And-Conquer Implementation}
    \end{subfigure}
    \caption{Binomial Tree and Divide-And-Conquer \texttt{Gather} with $root = 2$}
    \label{fig:gather:binom_vs_dac:root2}
\end{figure}

\noindent As seen in Figure \ref{fig:gather:binom_vs_dac:root0}, both algorithms perform similarly when the root node has rank $0$. But when using rank $2$ as the root, the reordering of data in the Binomial-Tree implementations leads to an overhead and therefore the Divide-And-Conquer algorithm performs better, as seen in Figure \ref{fig:gather:binom_vs_dac:root2}.\\
Because the following benchmarks are all done with $root = 0$, the binomial-tree implementation was chosen as the main strategy.

%-------
\subsection{Description / Strategy}

All processing nodes are organized in a binomial-tree with the root of the \texttt{Gather} algorithm as the root of the tree. Each leaf node sends its data to its parent node and all non-leaf nodes forward all received data as well as the local data of the node itself to the parent. \\
Due to the use of a binomial tree, the calculation of the parent and child ranks can be done fast via bit-operations.\\

\noindent For cases where the root node has $rank \neq 0$, the algorithm uses virtual ranks:
$vrank = (rank - root - N ) \mod N$. In this case the data is received in virtual rank order at the root and has to be reordered to regain the real rank order.

\subsection{Round- and Bandwidth Optimality}

\subsubsection{Round Optimality}
Due to the tree structure nodes on the same level send data to their parent nodes in parallel, so each layer of the tree represents one communication round. Therefore the number of communications rounds is limited by $\mathcal{O}(log(p))$.

\subsubsection{Bandwidth Optimality}
Per round at most $2^{log(p)-1} \frac{m}{p}$ units of data are sent. % TODO

%-------
\subsection{Harmful Algorithmic Latency}
No Algorithmic Latency: Due to the calculation of child and parent nodes via bit operations, each processor can start to send/receive after $\mathcal{O}(1)$ steps.


%----------------------------------------------------------------------
\section{Algorithm \myscatter}

%-------
\subsection{Strategy Comparison}
Again two \texttt{Scatter} algorithms one based on Binomial-Trees and one based on Divide-And-Conquer were implemented and compared. Figure \ref{fig:scatter:binom_vs_dac:root0} shows the performance of each implementation compared to the \texttt{Open MPI Scatter} implementation with $root = 0$. Figure \ref{fig:scatter:binom_vs_dac:root2} shows the same comparison with $root = 2$.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/scatter_32/runtime.pdf}
        \caption{Binomial-Tree Implementation}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/scatter_32/runtime.pdf}
        \caption{Divide-And-Conquer Implementation}
    \end{subfigure}
    \caption{Binomial Tree and Divide-And-Conquer \texttt{Scatter} with $root = 0$}
    \label{fig:scatter:binom_vs_dac:root0}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/root=2/binom/scatter_32/runtime.pdf}
        \caption{Binomial-Tree Implementation}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/root=2/divide_conquer/scatter_32/runtime.pdf}
        \caption{Divide-And-Conquer Implementation}
    \end{subfigure}
    \caption{Binomial Tree and Divide-And-Conquer \texttt{Scatter} with $root = 2$}
    \label{fig:scatter:binom_vs_dac:root2}
\end{figure}

\noindent As before, the performance of the Binomial-Tree algorithm performs worse than the Divide-And-Conquer approach when using $rank \neq 0$, due to the reordering overhead on the \texttt{Scatter} root node. Also the Divide-And-Conquer algorithm performs better for medium and big message sizes.\\
Due to the better overall performance of the Divide-And-Conquer based algorithm, this implementation was  chosen as baseline implementation for the following discussions.


%-------
\subsection{Description / Strategy}

Divide-And-Conquer works by recursively breaking down a given problem into a number of sub-problems, in this case two sub-problems. The processes are split into two sets and the root nodes selects one process, denoted as \textit{subroot}, from the other set and sends half of the data to it. This is repeated for both sets until every node has received its data. On each communication round the number of senders is increased by a factor of two, while the amount of data to be send on each node is decreased by a factor of two.\newline

\noindent Cases where the \texttt{Scatter} root node has $rank \neq 0$ are already covered by this algorithm and thus don't require additional efforts. 

%-------
\subsection{Round- and Bandwidth Optimality}

\subsubsection{Round Optimality}

Using the Divide-And-Conquer approach, the data is distributed to the nodes along a binary tree, where all communication on the same tree-layer is executed at the same time. Therefore the number of communication rounds is limited by $\mathcal{O}(log(p))$.

\subsubsection{Bandwidth Optimality}



%-------
\subsection{Harmful Algorithmic Latency}

No Algorithmic Latency: Each Processor can start to send/receive after $\mathcal{O}(1)$ steps.

%----------------------------------------------------------------------
\section{Experimental Results}

%-------
\subsection{Experiments and Discussion -- \mygather}

\begin{figure}[H]
    \centering
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/gather_31/rel_runtime.pdf}
        \caption{Relative Runtime with $p=31 \times 16$}
        \label{fig:Gather:OpenMPI:Rel:31}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/gather_31/runtime.pdf}
        \caption{Absolute Runtime with $p=31 \times 16$}
        \label{fig:Gather:OpenMPI:Abs:31}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/gather_32/rel_runtime.pdf}
        \caption{Relative Runtime with $p=32 \times 16$}
        \label{fig:Gather:OpenMPI:Rel:32}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/gather_32/runtime.pdf}
        \caption{Absolute Runtime with $p=32 \times 16$}
        \label{fig:Gather:OpenMPI:Abs:32}
    \end{subfigure}
    
    \caption{\mygather compared to \mpigather of \texttt{Open MPI 1.10.3}}
\end{figure}

In both test cases, either with $512$ or with $496$ processes the implementations of \mygather and \mpigather (\texttt{Open MPI 1.10.3}) perform similarly in most of the cases. 
But with message sizes $> 100$ KByte the \mpigather implementation outperforms \mygather. We assume \mpigather implements some optimizations when dealing with big message sizes.

\begin{figure}[H]
    \centering
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/binom/gather_31/rel_runtime.pdf}
        \caption{Relative Runtime with $p=31 \times 16$}
        \label{fig:Gather:MPICH:Rel:31}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/binom/gather_31/runtime.pdf}
        \caption{Absolute Runtime with $p=31 \times 16$}
        \label{fig:Gather:MPICH:Abs:31}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/binom/gather_32/rel_runtime.pdf}
        \caption{Relative Runtime with $p=32 \times 16$}
        \label{fig:Gather:MPICH:Rel:32}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/binom/gather_32/runtime.pdf}
        \caption{Absolute Runtime with $p=32 \times 16$}
        \label{fig:Gather:MPICH:Abs:32}
    \end{subfigure}
    
    \caption{\mygather compared to \mpigather of \texttt{MVAPICH2 2.2}}
\end{figure}

\noindent Compared to the benchmark results before, the \mpigather implementation of \texttt{MVAPICH2 2.2} also slightly outperforms the \mygather implementations with smaller message sizes. The rather big latency changes when the message size exceeds $10$ KByte is also quite interesting, we assume that \texttt{MVAPICH2 2.2} switches to another algorithm when reaching a certain threshold.

%-------
\subsection{Experiments and Discussion -- \myscatter}

\begin{figure}[H]
    \centering
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/scatter_31/rel_runtime.pdf}
        \caption{Relative Runtime with $p=31 \times 16$}
        \label{fig:Scatter:OpenMPI:Rel:31}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/scatter_31/runtime.pdf}
        \caption{Absolute Runtime with $p=31 \times 16$}
        \label{fig:Scatter:OpenMPI:Abs:31}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/scatter_32/rel_runtime.pdf}
        \caption{Relative Runtime with $p=32 \times 16$}
        \label{fig:Scatter:OpenMPI:Rel:32}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/scatter_32/runtime.pdf}
        \caption{Absolute Runtime with $p=32 \times 16$}
        \label{fig:Scatter:OpenMPI:Abs:32}
    \end{subfigure}
    
    \caption{\myscatter compared to \mpiscatter of \texttt{Open MPI 1.10.3}}
\end{figure}

Compared to the \texttt{Open MPI 1.10.3} implementation of \mpiscatter, \myscatter achieves similar performance for smaller message sizes. 
For bigger message sizes \mpiscatter outperforms \myscatter, which is again assumed to be because of optimizations for bigger messages.
We haven't found a reason for the particular widespread latency distribution of \myscatter when the message size is $10$ kByte, as seen in Figure \ref{fig:Scatter:OpenMPI:Rel:32}. 


\begin{figure}[H]
    \centering
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/divide_conquer/scatter_31/rel_runtime.pdf}
        \caption{Relative Runtime with $p=31 \times 16$}
        \label{fig:Scatter:MPICH:Rel:31}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/divide_conquer/scatter_31/runtime.pdf}
        \caption{Absolute Runtime with $p=31 \times 16$}
        \label{fig:Scatter:MPICH:Abs:31}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/divide_conquer/scatter_32/rel_runtime.pdf}
        \caption{Relative Runtime with $p=32 \times 16$}
        \label{fig:Scatter:MPICH:Rel:32}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/divide_conquer/scatter_32/runtime.pdf}
        \caption{Absolute Runtime with $p=32 \times 16$}
        \label{fig:Scatter:MPICH:Abs:32}
    \end{subfigure}
    
    \caption{\myscatter compared to \mpiscatter of \texttt{MVAPICH2 2.2}}
\end{figure}

\noindent The \texttt{MVAPICH2 2.2} implementation of \mpigather outperforms \mygather in nearly all cases. The rather big latency changes when the message size exceeds $10$ KByte is also quite interesting, we assume that \texttt{MVAPICH2 2.2} switches to another algorithm when reaching a certain threshold.

