% !TEX root = template.tex

\newcommand\mygather{\texttt{MY\_Gather}\xspace}
\newcommand\myscatter{\texttt{MY\_Scatter}\xspace}

\newcommand\mpigather{\texttt{MPI\_Gather}\xspace}
\newcommand\mpiscatter{\texttt{MPI\_Scatter}\xspace}

%----------------------------------------------------------------------
\section{Algorithm \mygather}

%-------
\subsection{Strategy Comparison}

Two different Gather algorithms, one based on Binomial-Trees and one based on a Divide-And-Conquer approach were implemented and tested. Figure \ref{fig:gather:binom_vs_dac} shows the performance of each of the implementations compared to the OpenMPI Gather implementation, with root = 0 and root $\neq$ 0.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/gather_32/runtime.pdf}
        \caption{Binomial-Tree Implementation}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/gather32/runtime.pdf}
        \caption{Divide-And-Conquer Implementation}
    \end{subfigure}
    \caption{Binomial Tree and Divide-And-Conquer Implementation with root = 0}
    \label{fig:gather:binom_vs_dac}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/root=2/binom/gather_32/runtime.pdf}
        \caption{Binomial-Tree Implementation}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/root=2/divide_conquer/gather_32/runtime.pdf}
        \caption{Divide-And-Conquer Implementation}
    \end{subfigure}
    \caption{Binomial Tree and Divide-And-Conquer Implementation with root = 2}
    \label{fig:scatter:binom_vs_dac}
\end{figure}

As seen, both algorithms perform similarly when the root node has rank 0. But when using the node with rank 2 as the root, the reordering of data in the Binomial-Tree implementations leads to an overhead and therefore the Divide-And-Conquer algorithm performs better.\\
Because the following benchmarks are all done with root = 2, the binomial-tree implementation was chosen as the main strategy.

%-------
\subsection{Description / Strategy}

All processing nodes are organized in a binomial-tree with the root of the Gather algorithm as the root of the tree. Each leaf node sends its data to its parent node and all non-leaf nodes forward all received data and the data of the node itself to the parent. \\
Due to the use of a binomial tree, the calculation of the parent and child ranks can be done fast via bit-operations.\\

For cases where the root node has $rank \neq 0$, the algorithm uses virtual ranks:
$vrank = (rank - root - N ) \mod N$. In this case the data is received in virtual rank order at the root and has to be reordered to regain the real rank order.

\subsection{Round- and Bandwidth Optimality}

\subsubsection{Round Optimality}
Due to the tree structure nodes on the same level send data to their parent nodes in parallel, so each layer of the tree represents one communication round. Therefore the number of communications rounds is limited by $\mathcal{O}(log(p))$.

\subsubsection{Bandwidth Optimality}
Per round at most $2^{log(p)-1} \frac{m}{p}$ units of data are sent. % TODO

%-------
\subsection{Harmful Algorithmic Latency}
No Algorithmic Latency: Due to the calculation of child and parent nodes via bit operations, each processor can start to send/receive after $\mathcal{O}(1)$ steps.


%----------------------------------------------------------------------
\section{Algorithm \myscatter}

%-------
\subsection{Strategy Comparison}
Again two algorithms one based on Binomial-Trees and one based on Divide-And-Conquer were tested and a comparison can be seen in Figure \ref{fig:scatter:binom_vs_dac}.

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/scatter_32/runtime.pdf}
        \caption{Binomial-Tree Implementation}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/scatter_32/runtime.pdf}
        \caption{Divide-And-Conquer Implementation}
    \end{subfigure}
    \caption{Binomial Tree and Divide-And-Conquer Implementation with root = 0}
    \label{fig:scatter:binom_vs_dac}
\end{figure}

\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/root=2/binom/scatter_32/runtime.pdf}
        \caption{Binomial-Tree Implementation}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/root=2/divide_conquer/scatter_32/runtime.pdf}
        \caption{Divide-And-Conquer Implementation}
    \end{subfigure}
    \caption{Binomial Tree and Divide-And-Conquer Implementation with root = 2}
    \label{fig:scatter:binom_vs_dac}
\end{figure}

As before, the performance of the Binomial-Tree algorithm performs worse than the Divide-And-Conquer approach when using rank $\neq$ 0, due to the reordering overhead.\\
Due to the better performance the Divide-And-Conquer implementation is used as baseline implementation for the following discussions.


%-------
\subsection{Description / Strategy}

The processes are split into two sets and the root nodes selects one process (subroot) from the other set and sends half of the data to it. This is repeated for both sets until every node has received its data.

%-------
\subsection{Round- and Bandwidth Optimality}

\subsubsection{Round Optimality}
Using the Divide-And-Conquer approach, the data is distributed to the nodes along a binary tree, where all communication on the same tree-layer is executed at the same time. Therefore the number of communication rounds is limited by $\mathcal{O}(log(p))$.

\subsubsection{Bandwidth Optimality}



%-------
\subsection{Harmful Algorithmic Latency}

No Algorithmic Latency: Each Processor can start to send/receive after $\mathcal{O}(1)$ steps.

%----------------------------------------------------------------------
\section{Experimental Results}

%-------
\subsection{Experiments and Discussion -- \mygather}

\begin{figure}[H]
    \centering
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/gather_31/rel_runtime.pdf}
        \caption{Relative Runtime with $p=31 \times 16$}
        \label{fig:Gather:OpenMPI:Rel:31}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/gather_31/runtime.pdf}
        \caption{Absolute Runtime with $p=31 \times 16$}
        \label{fig:Gather:OpenMPI:Abs:31}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/gather_32/rel_runtime.pdf}
        \caption{Relative Runtime with $p=32 \times 16$}
        \label{fig:Gather:OpenMPI:Rel:32}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/binom/gather_32/runtime.pdf}
        \caption{Absolute Runtime with $p=32 \times 16$}
        \label{fig:Gather:OpenMPI:Abs:32}
    \end{subfigure}
    
    \caption{\mygather compared to \mpigather of \texttt{Open MPI 1.10.3}}
\end{figure}

In both test cases, either with 512 or with 496 processes the implementations of \mygather and \mpigather (\texttt{Open MPI 1.10.3}) perform for the most cases similarly.
But with message sizes > 100 KByte the \mpigather implementation outperforms \mygather. We assume \mpigather implements some optimizations when dealing with big message sizes.

\begin{figure}[H]
    \centering
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/binom/gather_31/rel_runtime.pdf}
        \caption{Relative Runtime with $p=31 \times 16$}
        \label{fig:Gather:MPICH:Rel:31}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/binom/gather_31/runtime.pdf}
        \caption{Absolute Runtime with $p=31 \times 16$}
        \label{fig:Gather:MPICH:Abs:31}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/binom/gather_32/rel_runtime.pdf}
        \caption{Relative Runtime with $p=32 \times 16$}
        \label{fig:Gather:MPICH:Rel:32}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/binom/gather_32/runtime.pdf}
        \caption{Absolute Runtime with $p=32 \times 16$}
        \label{fig:Gather:MPICH:Abs:32}
    \end{subfigure}
    
    \caption{\mygather compared to \mpigather of \texttt{MVAPICH2 2.2}}
\end{figure}

Compared to the benchmark results before, the \mpigather implementation of \texttt{MVAPICH2 2.2} also slightly outperforms the \mygather implementations with smaller message sizes.

%-------
\subsection{Experiments and Discussion -- \myscatter}

\begin{figure}[H]
    \centering
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/scatter_31/rel_runtime.pdf}
        \caption{Relative Runtime with $p=31 \times 16$}
        \label{fig:Scatter:OpenMPI:Rel:31}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/scatter_31/runtime.pdf}
        \caption{Absolute Runtime with $p=31 \times 16$}
        \label{fig:Scatter:OpenMPI:Abs:31}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/scatter_32/rel_runtime.pdf}
        \caption{Relative Runtime with $p=32 \times 16$}
        \label{fig:Scatter:OpenMPI:Rel:32}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/openmpi/divide_conquer/scatter_32/runtime.pdf}
        \caption{Absolute Runtime with $p=32 \times 16$}
        \label{fig:Scatter:OpenMPI:Abs:32}
    \end{subfigure}
    
    \caption{\myscatter compared to \mpiscatter of \texttt{Open MPI 1.10.3}}
\end{figure}

Compared to the \texttt{Open MPI 1.10.3} implementation of \mpiscatter, \myscatter achieves similar performance for smaller message sizes. 
For bigger message sizes \mpiscatter outperforms \myscatter, which is again assumed to be because of optimizations for bigger messages.


\begin{figure}[H]
    \centering
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/divide_conquer/scatter_31/rel_runtime.pdf}
        \caption{Relative Runtime with $p=31 \times 16$}
        \label{fig:Scatter:MPICH:Rel:31}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/divide_conquer/scatter_31/runtime.pdf}
        \caption{Absolute Runtime with $p=31 \times 16$}
        \label{fig:Scatter:MPICH:Abs:31}
    \end{subfigure}
    
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/divide_conquer/scatter_32/rel_runtime.pdf}
        \caption{Relative Runtime with $p=32 \times 16$}
        \label{fig:Scatter:MPICH:Rel:32}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=\textwidth]{../benchmarks/mpich/divide_conquer/scatter_32/runtime.pdf}
        \caption{Absolute Runtime with $p=32 \times 16$}
        \label{fig:Scatter:MPICH:Abs:32}
    \end{subfigure}
    
    \caption{\myscatter compared to \mpiscatter of \texttt{MVAPICH2 2.2}}
\end{figure}

The \texttt{MVAPICH2 2.2} implementation of \mpigather outperforms \mygather in nearly all cases.

